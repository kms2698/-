# 모델 평가를 위해 사용되는 지표
1. 정확도(accuracy)
2. 민감도(재현율)(recall)
3. 특이도(specificity)
4. 정밀도(precision)
5. F1 score


## 정확도
정확도는 정답과 예측값이 얼마나 동일한지를 판단하는 지표
직관적으로 모델 예측 성능을 나타내는 평가지표.

데이터 레이블의 구성에 따라 모델 성능을 왜곡할 수 있다.

예) kaggle의 타이타닉 예제에서 생존자 중 여성의 비중이 높기 때문에, 특별한 알고리즘 없이 여성을 모두 생존자로 분류해도 정확도가 높게 나올수있다.

따라서 정확도를 평가지표로 사용할 때는 신중해야 한다.
정확도는 특히, 불균형한 레이블 값 분포에서 모델 성능을 판단할 경우에는 적합한 평가지표가 아니다.

## 혼동행렬(Confusion Matrix)
혼동행렬은 분류문제에서 예측 오류가 얼마나 되고, 어떤 유형의 오류가 발생하는지를 보여주는 행렬
||Predicted 0|Predicted 1|
|:---:|:---:|:---:|
|Actual 0|TN|FP|
|Actual 1|FN|TP|

그 후, 학습셋과 검증데이터셋 전체를 다시 학습하여 테스트셋에 대해 예측을 수행한다. 이를 통해 학습하지 않은 데이터에 대해서도 모델의 성능이 잘 나오는지 확인함.

- TN(True Negative) : 실제 값이 음성(Negative)인데 예측 값도 음성(Negative) → 맞게 예측
- FP(False Positive) : 실제 값이 음성(Negative)인데 예측 값은 양성(Positive)
- FN(False Negative) : 실제 값이 양성(Positive)인데 예측 값은 음성(Negative)
- TP(True Positive) : 실제 값이 양성(Positive)인데 예측 값도 양성(Positive) → 맞게 예측

이를통해 알 수 있는 지표

1. 정확도(Accuracy) = ( TN + TP ) / ( TN + FP + FN + TP )
   - 전체 예측 중 양성을 양성이라 예측하고, 음성을 음성이라고 예측한 갯수의 비율.
   - 레이블의 분포에 따라 한쪽으로만 분류하더라도 결과가 높게 나올 수 있음.

2. 정밀도(Precision) = TP / ( FP + TP )
   - 양성으로 예측한 것 중 실제로 양성인 비율

3. 재현율(Recall) = TP / ( FN + TP )
   - 전체 양성 데이터 중 양성으로 예측한 수의 비율. 
   - 다른 말로 민감도라고도 하며 양성에 얼마나 민감하게 반응하느냐는 의미
   - 민감도(Sensitivity), 또는 TPR(True Positive Rate) 이라고도함

4. 특이도(Specificity) = TN / ( TN + FP )
   - 특이도는 전체 음성 데이터 중 음성으로 예측한 수의 비율
   - 음성을 얼마나 잘 골라내는지를 판정하는 지표.
   - TNR(True Negative Rate) 이라고도 함.

5. FPR(False Positive Rate)
   - 거짓양성비율(FalsePositiveRate)은 음성 중 양성으로 잘못 판정된 것의 비율

프로젝트의 목적에 따라 특정 지표가 유용하게 사용된다.

예) 암 환자 판별의 경우에는 정상인을 암환자로 분류하더라도, 암인 케이스를 반드시 판정해내야 한다. 따라서 재현율이 중시된다.

→ 우리도 당뇨병환자인 사람은 당뇨병으로 반드시 판정해야되나?

sklearn에 정확도, 재현율, 정밀도 계산을 위한 함수제공
- accuracy_score()
- recall_score()
- precision_score()

## 정밀도와 재현율의 trade-off관계
정밀도와 재현율은 상호 보완적인 지표임.
한쪽이 높아지면 다른 한쪽은 떨어지기 쉽다.

## F1 스코어
정밀도와 재현율을 결합한 지표

정밀도와 재현율이 어느 한쪽으로 치우치지 않을 때 상대적으로 높은 값을 가짐

$$
F1 = \frac{2}{\frac{1}{recall} + \frac{1}{precision}} = 2 * \frac{precision*recall}{precision+recall}
$$

- f1_score()


참고) https://sevillabk.github.io/model-evaluation/


